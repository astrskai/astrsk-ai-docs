"use strict";(self.webpackChunkastrsk_ai_docs=self.webpackChunkastrsk_ai_docs||[]).push([[570],{484:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>i,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>l,toc:()=>c});const l=JSON.parse('{"id":"provider/provider-ollama","title":"Connect Ollama","description":"Ollama is an application that allows you to run AI models directly on your local device.","source":"@site/docs/provider/provider-ollama.md","sourceDirName":"provider","slug":"/provider/provider-ollama","permalink":"/provider/provider-ollama","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/provider/provider-ollama.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Connect KoboldCPP","permalink":"/provider/provider-koboldcpp"},"next":{"title":"What are Providers?","permalink":"/provider/what-is-providers"}}');var t=n(4848),o=n(8453);const r={},s="Connect Ollama",i={},c=[{value:"Install Ollama",id:"install-ollama",level:2},{value:"Get Models",id:"get-models",level:2},{value:"Set Environment Variable",id:"set-environment-variable",level:2},{value:"Start Ollama",id:"start-ollama",level:2},{value:"Connect Ollama in astrsk.ai",id:"connect-ollama-in-astrskai",level:2}];function d(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.header,{children:(0,t.jsx)(a.h1,{id:"connect-ollama",children:"Connect Ollama"})}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.a,{href:"https://ollama.com/",children:"Ollama"})," is an application that allows you to run AI models directly on your local device."]}),"\n",(0,t.jsx)(a.h2,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,t.jsxs)(a.p,{children:["Refer to ",(0,t.jsx)(a.a,{href:"https://github.com/ollama/ollama/blob/main/README.md",children:"this document"})," for instructions on installing Ollama on your system."]}),"\n",(0,t.jsx)(a.h2,{id:"get-models",children:"Get Models"}),"\n",(0,t.jsxs)(a.p,{children:["From your terminal, run the following command to download the desired model (e.g., ",(0,t.jsx)(a.code,{children:"llama3.1:8b"}),"):"]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:"ollama pull llama3.1:8b\n"})}),"\n",(0,t.jsx)(a.h2,{id:"set-environment-variable",children:"Set Environment Variable"}),"\n",(0,t.jsxs)(a.p,{children:["To allow astrsk.ai to connect to your local Ollama instance, you need to set the ",(0,t.jsx)(a.code,{children:"OLLAMA_ORIGINS"})," environment variable. Refer to ",(0,t.jsx)(a.a,{href:"https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server",children:"this document"})," for instructions on setting environment variables for your operating system."]}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:'OLLAMA_ORIGINS="https://app.astrsk.ai"\n'})}),"\n",(0,t.jsxs)(a.p,{children:["After setting the environment variable, ",(0,t.jsx)(a.strong,{children:"restart your terminal or command prompt"})," for the changes to take effect."]}),"\n",(0,t.jsx)(a.h2,{id:"start-ollama",children:"Start Ollama"}),"\n",(0,t.jsx)(a.p,{children:"Start the Ollama server by clicking the Ollama application icon in your app list, or by running the following command in your terminal:"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:"ollama serve\n"})}),"\n",(0,t.jsx)(a.h2,{id:"connect-ollama-in-astrskai",children:"Connect Ollama in astrsk.ai"}),"\n",(0,t.jsxs)(a.p,{children:["Within astrsk.ai, navigate to the provider settings. Select ",(0,t.jsx)(a.strong,{children:"Ollama"})," as the source, verify the ",(0,t.jsx)(a.strong,{children:"Base URL"})," (usually ",(0,t.jsx)(a.code,{children:"http://localhost:11434/api"}),"), and then press ",(0,t.jsx)(a.strong,{children:"Connect"}),"."]}),"\n",(0,t.jsxs)(a.p,{children:[(0,t.jsx)(a.img,{alt:"Connect Ollama",src:n(649).A+"",width:"3284",height:"2046"}),"\n",(0,t.jsx)(a.em,{children:"[Image placeholder: Shows the Ollama connection interface in astrsk.ai settings.]"})]})]})}function m(e={}){const{wrapper:a}={...(0,o.R)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},649:(e,a,n)=>{n.d(a,{A:()=>l});const l=n.p+"assets/images/connect-ollama-879c51accffae14510414372cfed2cbc.png"},8453:(e,a,n)=>{n.d(a,{R:()=>r,x:()=>s});var l=n(6540);const t={},o=l.createContext(t);function r(e){const a=l.useContext(o);return l.useMemo(function(){return"function"==typeof e?e(a):{...a,...e}},[a,e])}function s(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),l.createElement(o.Provider,{value:a},e.children)}}}]);