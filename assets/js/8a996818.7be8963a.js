"use strict";(self.webpackChunkastrsk_ai_docs=self.webpackChunkastrsk_ai_docs||[]).push([[821],{2471:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-556a6f51edf8af6e8f9e2c2f00da2f3f.png"},4228:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-history-range-520da82956e0b0ba8e3b9c3be2f6b0b1.png"},6177:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-chat-completion-38fd9488475f76332040dd0079d7a111.png"},6428:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-text-completion-840c97018d6d4bdf59c7f8a736bb0782.png"},6774:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-history-message-ee1d89d2a16df0a6a825e385e7b81d24.png"},8453:(e,t,s)=>{s.d(t,{R:()=>a,x:()=>r});var n=s(6540);const o={},i=n.createContext(o);function a(e){const t=n.useContext(i);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),n.createElement(i.Provider,{value:t},e.children)}},8672:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"flow/node/agent-node/prompt","title":"Prompt","description":"A prompt is the core input value passed to the model. In the context of Large Language Models (LLMs), a prompt is a text instruction or question that guides the model to generate a desired response. It serves as the primary way to communicate with the AI model and influence its behavior and output.","source":"@site/docs/flow/node/agent-node/prompt.md","sourceDirName":"flow/node/agent-node","slug":"/flow/node/agent-node/prompt","permalink":"/astrsk-ai-docs/flow/node/agent-node/prompt","draft":false,"unlisted":false,"editUrl":"https://github.com/harpychat/astrsk-ai-docs/blob/main/docs/flow/node/agent-node/prompt.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1},"sidebar":"tutorialSidebar","previous":{"title":"Agent node","permalink":"/astrsk-ai-docs/flow/node/agent-node/"},"next":{"title":"Parameters","permalink":"/astrsk-ai-docs/flow/node/agent-node/parameters"}}');var o=s(4848),i=s(8453);const a={sidebar_position:1},r="Prompt",d={},c=[{value:"Chat Completion",id:"chat-completion",level:2},{value:"Message",id:"message",level:3},{value:"History Message",id:"history-message",level:3},{value:"History Range",id:"history-range",level:4},{value:"Text Completion",id:"text-completion",level:2}];function l(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.header,{children:(0,o.jsx)(t.h1,{id:"prompt",children:"Prompt"})}),"\n",(0,o.jsx)(t.p,{children:"A prompt is the core input value passed to the model. In the context of Large Language Models (LLMs), a prompt is a text instruction or question that guides the model to generate a desired response. It serves as the primary way to communicate with the AI model and influence its behavior and output."}),"\n",(0,o.jsx)(t.p,{children:"Prompts are categorized into two main formats based on their input structure:"}),"\n",(0,o.jsxs)(t.ul,{children:["\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Chat Completion"}),", which organizes input into structured message units."]}),"\n",(0,o.jsxs)(t.li,{children:[(0,o.jsx)(t.strong,{children:"Text Completion"}),", which accepts input as plain text. The availability of these input formats may vary depending on the model provider."]}),"\n"]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Prompt",src:s(2471).A+"",width:"668",height:"769"})}),"\n",(0,o.jsx)(t.h2,{id:"chat-completion",children:"Chat Completion"}),"\n",(0,o.jsxs)(t.p,{children:["Chat completion is a conversational format where messages are structured as a dialogue between different roles, primarily ",(0,o.jsx)(t.code,{children:"assistant"})," (AI, model), ",(0,o.jsx)(t.code,{children:"user"}),", and ",(0,o.jsx)(t.code,{children:"system"}),". The ",(0,o.jsx)(t.code,{children:"system"})," role is used to provide initial instructions or context that guides the assistant's behavior throughout the conversation - it sets the tone, personality, and operational guidelines for the AI. This format allows for more nuanced interactions by maintaining conversation context and role-based messaging. It's particularly effective for creating natural, back-and-forth conversations and is widely supported by modern language models."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Chat Completion",src:s(6177).A+"",width:"1326",height:"1039"})}),"\n",(0,o.jsx)(t.h3,{id:"message",children:"Message"}),"\n",(0,o.jsxs)(t.p,{children:["Messages are the individual units of communication in chat completion format. Each message consists of a role (",(0,o.jsx)(t.code,{children:"system"}),", ",(0,o.jsx)(t.code,{children:"user"}),", or ",(0,o.jsx)(t.code,{children:"assistant"}),") and content (the actual text). Messages allow you to craft specific instructions, questions, or responses that will be sent to the model as part of the conversation context."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Message",src:s(9445).A+"",width:"1326",height:"1039"})}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.strong,{children:"Examples:"})}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{className:"language-json",children:'[\n  {\n    "role": "system",\n    "content": "You are a helpful assistant that speaks in a friendly tone."\n  },\n  {\n    "role": "user",\n    "content": "What\'s the weather like today?"\n  },\n  {\n    "role": "assistant",\n    "content": "I\'d be happy to help! However, I don\'t have access to real-time weather data."\n  }\n]\n'})}),"\n",(0,o.jsx)(t.h3,{id:"history-message",children:"History Message"}),"\n",(0,o.jsx)(t.p,{children:"History messages are used to insert past conversation history from the session into the prompt. This provides the model with context about previous interactions, helping it generate more coherent and contextually appropriate responses."}),"\n",(0,o.jsxs)(t.p,{children:["The role of each message is automatically determined based on the speaker; messages spoken by the character for whom you want to generate a response through the flow are set to ",(0,o.jsx)(t.code,{children:"assistant"}),", while messages spoken by other characters are set to ",(0,o.jsx)(t.code,{children:"user"}),"."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"History Message",src:s(6774).A+"",width:"1329",height:"1040"})}),"\n",(0,o.jsx)(t.h4,{id:"history-range",children:"History Range"}),"\n",(0,o.jsx)(t.p,{children:"You can include all past conversation history in the prompt using history messages. However, this would consume too many tokens, leading to increased costs. Alternatively, depending on the agent's purpose, you may only need specific messages rather than the entire conversation."}),"\n",(0,o.jsx)(t.p,{children:"Through History Range, you can specify a range starting from either the oldest or newest messages to include in the prompt. This helps manage token usage and focus on relevant conversation context. For example, let's say the session's history is in the following state:"}),"\n",(0,o.jsx)(t.pre,{children:(0,o.jsx)(t.code,{children:"[1] User: Hello! How are you today?\n[2] AI: I'm doing well, thank you for asking! How can I help you?\n[3] User: I'm looking for a good book recommendation.\n[4] AI: What genre are you interested in? Fiction, non-fiction, mystery?\n[5] User: I love science fiction novels.\n"})}),"\n",(0,o.jsxs)(t.p,{children:["To retrieve the 3 most recent messages (messages 3, 4, and 5), you would set the history range to ",(0,o.jsx)(t.code,{children:"Count from end"})," and specify a range from ",(0,o.jsx)(t.code,{children:"0"})," to ",(0,o.jsx)(t.code,{children:"3"}),". This would include the last 3 messages in your prompt while excluding the earlier conversation history."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"History Range",src:s(4228).A+"",width:"660",height:"415"})}),"\n",(0,o.jsx)(t.h2,{id:"text-completion",children:"Text Completion"}),"\n",(0,o.jsx)(t.p,{children:"Text completion is a simpler prompt format where you provide a single block of text as input to the model. The model works by completing or continuing the input text that was provided. Unlike chat completion, it doesn't use structured messages with roles, but instead works with raw text continuation."}),"\n",(0,o.jsx)(t.p,{children:"This format was commonly used in early LLMs, but many modern providers have deprecated it in favor of the more structured Chat Completion format. When available, text completion can be useful for tasks like story writing, document continuation, or simple text generation where conversational structure isn't needed."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Text Completion",src:s(6428).A+"",width:"1326",height:"1037"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},9445:(e,t,s)=>{s.d(t,{A:()=>n});const n=s.p+"assets/images/prompt-message-c57d7e196a85705af8304f07a2beb03b.png"}}]);